{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245314eb",
   "metadata": {},
   "source": [
    "### Code from teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358895cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal decoder-only Transformer blocks in Flax/JAX, commented for learning.\n",
    "\n",
    "The model mirrors a GPT-style architecture:\n",
    "- Token embeddings + learned positional embeddings\n",
    "- Stack of Pre-LayerNorm decoder blocks with causal self-attention\n",
    "- Final LayerNorm\n",
    "- Weight tying between input embeddings and output logits projection\n",
    "\n",
    "Tensor shape conventions used below:\n",
    "- B: batch size\n",
    "- T: sequence length (time/positions)\n",
    "- D: hidden size / embedding dimension (d_model)\n",
    "- V: vocabulary size\n",
    "\"\"\"\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen import attention as attn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "        \"\"\"Transformer feed-forward network (a.k.a. MLP block).\n",
    "\n",
    "        Structure: Dense(D -> 4D), GELU, Dense(4D -> D) by default.\n",
    "        The expansion factor can be adjusted with `mlp_ratio`.\n",
    "\n",
    "        Args:\n",
    "            d_model: Hidden size D.\n",
    "            mlp_ratio: Expansion factor for the intermediate hidden size.\n",
    "\n",
    "        Input shape:  (B, T, D)\n",
    "        Output shape: (B, T, D)\n",
    "        \"\"\"\n",
    "\n",
    "        d_model: int\n",
    "        mlp_ratio: int = 4\n",
    "\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "                # Expand channel dimension (D -> hidden), apply non-linearity, project back to D.\n",
    "                hidden = int(self.d_model * self.mlp_ratio)\n",
    "                x = nn.Dense(hidden)(x)\n",
    "                x = nn.gelu(x)\n",
    "                x = nn.Dense(self.d_model)(x)\n",
    "                return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"A single decoder block (Pre-LayerNorm + Self-Attn + MLP + residuals).\n",
    "\n",
    "    Pre-LayerNorm improves training stability. Residual connections are used after\n",
    "    attention and MLP sublayers. The attention is causal when a causal mask is passed\n",
    "    (so each position can only attend to previous or current positions).\n",
    "\n",
    "    Args:\n",
    "      d_model: Hidden size D.\n",
    "      n_heads: Number of attention heads.\n",
    "\n",
    "    Input/Output shape: (B, T, D)\n",
    "    \"\"\"\n",
    "\n",
    "    d_model: int\n",
    "    n_heads: int\n",
    "    mlp_ratio: int = 4\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, mask=None):\n",
    "        # Attention sublayer: Pre-LayerNorm -> Self-Attention -> Residual add\n",
    "        h = nn.LayerNorm()(x)\n",
    "        h = nn.SelfAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            use_bias=False,\n",
    "        )(h, mask=mask)\n",
    "        x = x + h  # residual connection\n",
    "\n",
    "        # MLP sublayer: Pre-LayerNorm -> MLP -> Residual add\n",
    "        h = nn.LayerNorm()(x)\n",
    "        h = MLP(self.d_model, mlp_ratio=self.mlp_ratio)(h)\n",
    "        x = x + h  # residual connection\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"GPT-style decoder-only Transformer for language modeling.\n",
    "\n",
    "    Components:\n",
    "      - Token embeddings: maps token ids to D-dim vectors\n",
    "      - Learned positional embeddings: adds position information (0..T-1)\n",
    "      - N stacked decoder blocks with causal self-attention\n",
    "      - Final LayerNorm\n",
    "      - Output projection:\n",
    "          * If tie_weights=True (default), reuse token embedding matrix E to\n",
    "            compute logits via x @ E^T (implemented via einsum).\n",
    "          * Else, use a separate linear head to project to V logits.\n",
    "\n",
    "    Args:\n",
    "      vocab_size: Vocabulary size V.\n",
    "      d_model: Hidden size D.\n",
    "      n_layers: Number of decoder blocks.\n",
    "      n_heads: Attention heads per block.\n",
    "      max_len: Maximum supported sequence length for positional embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    max_len: int\n",
    "    mlp_ratio: int = 4\n",
    "\n",
    "    def setup(self):\n",
    "        # Token embedding table E with shape (V, D)\n",
    "        self.tok_embed = nn.Embed(self.vocab_size, self.d_model)\n",
    "\n",
    "        # Learned positional embeddings P with shape (max_len, D)\n",
    "        # We'll slice P[:T] each forward pass and add to token embeddings.\n",
    "        self.positional_embed = self.param(\n",
    "            \"positional_embed\",\n",
    "            nn.initializers.normal(stddev=0.02),\n",
    "            (self.max_len, self.d_model)\n",
    "        )\n",
    "\n",
    "        # Stack of decoder blocks\n",
    "        self.blocks = [DecoderBlock(d_model=self.d_model, n_heads=self.n_heads, mlp_ratio=self.mlp_ratio) for _ in range(self.n_layers)]\n",
    "\n",
    "        # Final LayerNorm before projecting to logits\n",
    "        self.layerNorm_final = nn.LayerNorm()\n",
    "\n",
    "        # Optional separate output head if not weight-tying\n",
    "        self.project_to_vocab = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        \"\"\"Forward pass (causal-only).\n",
    "\n",
    "        Args:\n",
    "          idx: Token ids of shape (B, T), dtype int32/int64.\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, T, V) unnormalized vocabulary scores for next-token prediction.\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token + positional embeddings -> (B, T, D)\n",
    "        x = self.tok_embed(idx) + self.positional_embed[:T]\n",
    "\n",
    "        # Build attention mask: strictly causal (lower-triangular), no padding mask.\n",
    "        causal = attn.make_causal_mask(jnp.ones((B, T), dtype=bool))\n",
    "        mask = causal\n",
    "\n",
    "        # Run the stack of decoder blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=mask)\n",
    "\n",
    "        # Final LayerNorm before output projection\n",
    "        x = self.layerNorm_final(x)\n",
    "\n",
    "        # Output projection to logits over V tokens.\n",
    "        logits = self.project_to_vocab(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2a121",
   "metadata": {},
   "source": [
    "### RoPE + multi-query attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    dim: int\n",
    "    eps: float = 1e-8\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        scale = self.param('scale', nn.initializers.ones, (self.dim,))\n",
    "        rms = jnp.sqrt(jnp.mean(jnp.square(x), axis=-1, keepdims=True) + self.eps)\n",
    "        return x * (scale / rms)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    d_model: int\n",
    "    mult: float = 2.667\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        hidden = int(self.d_model * self.mult)\n",
    "        u = nn.Dense(hidden, use_bias=False)(x)\n",
    "        v = nn.Dense(hidden, use_bias=False)(x)\n",
    "        x = nn.silu(u) * v\n",
    "        x = nn.Dropout(self.dropout)(x, deterministic=deterministic)\n",
    "        x = nn.Dense(self.d_model, use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "def rotary_emb(d: int, T: int, base: float = 10000.0, dtype=jnp.float32):\n",
    "    inv_freq = 1.0 / (base ** (jnp.arange(0, d, 2, dtype=dtype) / d))\n",
    "    t = jnp.arange(T, dtype=dtype)\n",
    "    freqs = jnp.einsum('t,f->tf', t, inv_freq)\n",
    "    cos, sin = jnp.cos(freqs), jnp.sin(freqs)\n",
    "    return cos[None, None, ...], sin[None, None, ...]\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    x1, x2 = jnp.split(x, 2, axis=-1)\n",
    "    xc = x1 * cos - x2 * sin\n",
    "    xs = x1 * sin + x2 * cos\n",
    "    return jnp.concatenate([xc, xs], axis=-1)\n",
    "\n",
    "\n",
    "class MQSelfAttention(nn.Module):\n",
    "    d_model: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int = 1\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, mask=None, deterministic: bool = True):\n",
    "        H = self.n_heads\n",
    "        H_kv = self.n_kv_heads\n",
    "        Dh = self.d_model // H\n",
    "\n",
    "        q = nn.Dense(self.d_model, use_bias=False)(x)\n",
    "        k = nn.Dense(H_kv * Dh, use_bias=False)(x)\n",
    "        v = nn.Dense(H_kv * Dh, use_bias=False)(x)\n",
    "\n",
    "        B, T, _ = q.shape\n",
    "        q = q.reshape(B, T, H, Dh).transpose(0, 2, 1, 3)\n",
    "        k = k.reshape(B, T, H_kv, Dh).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, T, H_kv, Dh).transpose(0, 2, 1, 3)\n",
    "\n",
    "        cos, sin = rotary_emb(Dh, T)\n",
    "        q = apply_rope(q, cos, sin)\n",
    "        k = apply_rope(k, cos, sin)\n",
    "\n",
    "        if H_kv != H:\n",
    "            factor = H // H_kv\n",
    "            k = jnp.repeat(k, repeats=factor, axis=1)\n",
    "            v = jnp.repeat(v, repeats=factor, axis=1)\n",
    "\n",
    "        scale = (1.0 / jnp.sqrt(jnp.array(Dh, dtype=x.dtype)))\n",
    "        att = jnp.einsum('bhtd,bhTd->bhtT', q, k) * scale\n",
    "\n",
    "        if mask is not None:\n",
    "            att = jnp.where(mask, att, jnp.finfo(att.dtype).min)\n",
    "        att = nn.softmax(att, axis=-1)\n",
    "        att = nn.Dropout(self.dropout)(att, deterministic=deterministic)\n",
    "\n",
    "        y = jnp.einsum('bhtT,bhTd->bhtd', att, v)\n",
    "        y = y.transpose(0, 2, 1, 3).reshape(B, T, self.d_model)\n",
    "        y = nn.Dense(self.d_model, use_bias=False)(y)\n",
    "        y = nn.Dropout(self.dropout)(y, deterministic=deterministic)\n",
    "        return y\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    d_model: int; n_heads: int; n_kv_heads: int = 1\n",
    "    attn_dropout: float = 0.1\n",
    "    mlp_dropout: float = 0.1\n",
    "    resid_scale_init: float = 1e-2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, mask=None, deterministic: bool = True):\n",
    "        scale = self.param('res_scale', nn.initializers.constant(self.resid_scale_init), ())\n",
    "\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = MQSelfAttention(self.d_model, self.n_heads, self.n_kv_heads, dropout=self.attn_dropout)(\n",
    "            h, mask=mask, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = SwiGLU(self.d_model, mult=2.667, dropout=self.mlp_dropout)(h, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int = 1\n",
    "    max_len: int = 2048\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    def setup(self):\n",
    "        self.tok_embed = nn.Embed(self.vocab_size, self.d_model)\n",
    "        self.blocks = [DecoderBlock(self.d_model, self.n_heads, self.n_kv_heads,\n",
    "                                    attn_dropout=self.dropout, mlp_dropout=self.dropout)\n",
    "                       for _ in range(self.n_layers)]\n",
    "        self.final_norm = RMSNorm(self.d_model)\n",
    "\n",
    "    def __call__(self, idx, *, deterministic: bool = True, pad_mask: jnp.ndarray | None = None):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_embed(idx)\n",
    "        x = nn.Dropout(self.dropout)(x, deterministic=deterministic)\n",
    "\n",
    "        causal = attn.make_causal_mask(jnp.ones((B, T), dtype=bool))\n",
    "        mask = causal if pad_mask is None else attn.combine_masks(\n",
    "            causal, attn.make_attention_mask(pad_mask, pad_mask, dtype=bool))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=mask, deterministic=deterministic)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = jnp.einsum('btd,vd->btv', x, self.tok_embed.embedding)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f3edf",
   "metadata": {},
   "source": [
    "### Transformer-XL with segment memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e85082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen import attention as attn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    dim: int\n",
    "    eps: float = 1e-8\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        g = self.param('scale', nn.initializers.ones, (self.dim,))\n",
    "        rms = jnp.sqrt(jnp.mean(x * x, axis=-1, keepdims=True) + self.eps)\n",
    "        return x * (g / rms)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    d_model: int\n",
    "    mult: float = 2.667\n",
    "    dropout: float = 0.1\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        h = int(self.d_model * self.mult)\n",
    "        u = nn.Dense(h, use_bias=False)(x)\n",
    "        v = nn.Dense(h, use_bias=False)(x)\n",
    "        y = nn.silu(u) * v\n",
    "        y = nn.Dropout(self.dropout)(y, deterministic=deterministic)\n",
    "        return nn.Dense(self.d_model, use_bias=False)(y)\n",
    "\n",
    "class XLBlock(nn.Module):\n",
    "    d_model: int\n",
    "    n_heads: int\n",
    "    mlp_mult: float = 2.667\n",
    "    attn_dropout: float = 0.1\n",
    "    mlp_dropout: float = 0.1\n",
    "    resid_scale_init: float = 1e-2\n",
    "    mem_len: int = 512\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, mem=None, deterministic: bool = True):\n",
    "        B, T, D = x.shape\n",
    "        mem = jnp.zeros((B, 0, D), x.dtype) if mem is None else mem\n",
    "        cat = jnp.concatenate([mem, x], axis=1)\n",
    "        M = mem.shape[1]\n",
    "\n",
    "        scale = self.param('res_scale', nn.initializers.constant(self.resid_scale_init), ())\n",
    "\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        q = nn.Dense(self.d_model, use_bias=False)(h)\n",
    "        kv = nn.Dense(2 * self.d_model, use_bias=False)(RMSNorm(self.d_model)(cat))\n",
    "        k, v = jnp.split(kv, 2, axis=-1)\n",
    "\n",
    "        H = self.n_heads\n",
    "        Dh = self.d_model // H\n",
    "        q = q.reshape(B, T, H, Dh).transpose(0, 2, 1, 3)\n",
    "        k = k.reshape(B, M + T, H, Dh).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, M + T, H, Dh).transpose(0, 2, 1, 3)\n",
    "\n",
    "        base = jnp.ones((1, 1, T, M + T), dtype=bool)\n",
    "        tri = jnp.tril(jnp.ones((T, T), dtype=bool))\n",
    "        mask = base.at[:, :, :, M:].set(tri[None, None, :, :])\n",
    "\n",
    "        att_logits = jnp.einsum('bhtd,bhTd->bhtT', q, k) / jnp.sqrt(Dh)\n",
    "        att_logits = jnp.where(mask, att_logits, jnp.finfo(att_logits.dtype).min)\n",
    "        att_probs = nn.softmax(att_logits, axis=-1)\n",
    "        att_probs = nn.Dropout(self.attn_dropout)(att_probs, deterministic=deterministic)\n",
    "        y = jnp.einsum('bhtT,bhTd->bhtd', att_probs, v)\n",
    "        y = y.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "        y = nn.Dense(self.d_model, use_bias=False)(y)\n",
    "        x = x + scale * y\n",
    "\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        y = SwiGLU(self.d_model, mult=self.mlp_mult, dropout=self.mlp_dropout)(h, deterministic=deterministic)\n",
    "        x = x + scale * y\n",
    "\n",
    "        new_mem = jax.lax.stop_gradient(jnp.concatenate([mem, x], axis=1))[:, -(self.mem_len):, :]\n",
    "        return x, new_mem\n",
    "\n",
    "class XLMemoryTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    mem_len: int = 512\n",
    "    dropout: float = 0.1\n",
    "    mlp_mult: float = 2.667\n",
    "\n",
    "    def setup(self):\n",
    "        self.tok_embed = nn.Embed(self.vocab_size, self.d_model)\n",
    "        self.blocks = [XLBlock(self.d_model, self.n_heads, self.mlp_mult,\n",
    "                               attn_dropout=self.dropout, mlp_dropout=self.dropout,\n",
    "                               mem_len=self.mem_len) for _ in range(self.n_layers)]\n",
    "        self.final_norm = RMSNorm(self.d_model)\n",
    "\n",
    "    def __call__(self, idx, *, mems=None, deterministic: bool = True):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_embed(idx)\n",
    "        x = nn.Dropout(self.dropout)(x, deterministic=deterministic)\n",
    "\n",
    "        if mems is None:\n",
    "            mems = [None] * self.n_layers\n",
    "        new_mems = []\n",
    "        for blk, mem in zip(self.blocks, mems):\n",
    "            x, mem_out = blk(x, mem=mem, deterministic=deterministic)\n",
    "            new_mems.append(mem_out)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = jnp.einsum('btd,vd->btv', x, self.tok_embed.embedding)\n",
    "        return logits, new_mems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5f556",
   "metadata": {},
   "source": [
    "### Transformer with a Switch/Top-1 MoE MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen import attention as attn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    dim: int\n",
    "    eps: float = 1e-8\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        g = self.param('scale', nn.initializers.ones, (self.dim,))\n",
    "        rms = jnp.sqrt(jnp.mean(x * x, axis=-1, keepdims=True) + self.eps)\n",
    "        return x * (g / rms)\n",
    "\n",
    "class SwitchMoE(nn.Module):\n",
    "    d_model: int\n",
    "    mult: float = 2.667\n",
    "    n_experts: int = 8\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        B, T, D = x.shape\n",
    "        H = int(D * self.mult)\n",
    "        router_w = self.param('router_w', nn.initializers.normal(0.02), (D, self.n_experts))\n",
    "        logits = jnp.einsum('btd,de->bte', x, router_w)\n",
    "        gates = nn.softmax(logits, axis=-1)\n",
    "        top1 = jnp.argmax(logits, axis=-1)\n",
    "\n",
    "        def expert():\n",
    "            return nn.Sequential([\n",
    "                nn.Dense(H, use_bias=False),\n",
    "                nn.silu,\n",
    "                nn.Dense(D, use_bias=False),\n",
    "            ])\n",
    "        experts = [expert() for _ in range(self.n_experts)]\n",
    "\n",
    "        out = jnp.zeros_like(x)\n",
    "        for e, ff in enumerate(experts):\n",
    "            mask = (top1 == e)[..., None]\n",
    "            xe = jnp.where(mask, x, 0.0)\n",
    "            ye = ff(xe)\n",
    "            ye = nn.Dropout(self.dropout)(ye, deterministic=deterministic)\n",
    "            out = out + jnp.where(mask, ye, 0.0)\n",
    "\n",
    "        expert_usage = jnp.mean(gates, axis=(0, 1))         \n",
    "        aux_loss = jnp.sum((expert_usage - 1.0 / self.n_experts) ** 2)\n",
    "        return out, aux_loss\n",
    "\n",
    "class DecoderBlockMoE(nn.Module):\n",
    "    d_model: int; n_heads: int\n",
    "    attn_dropout: float = 0.1\n",
    "    moe_dropout: float = 0.1\n",
    "    moe_mult: float = 2.667\n",
    "    n_experts: int = 8\n",
    "    resid_scale_init: float = 1e-2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, mask=None, deterministic: bool = True):\n",
    "        scale = self.param('res_scale', nn.initializers.constant(self.resid_scale_init), ())\n",
    "\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = nn.SelfAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            use_bias=False,\n",
    "            dropout_rate=self.attn_dropout,\n",
    "        )(h, mask=mask, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        y, aux = SwitchMoE(\n",
    "            self.d_model, mult=self.moe_mult,\n",
    "            n_experts=self.n_experts, dropout=self.moe_dropout\n",
    "        )(h, deterministic=deterministic)\n",
    "        x = x + scale * y\n",
    "        return x, aux\n",
    "\n",
    "class MoETransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_experts: int = 8\n",
    "    dropout: float = 0.1\n",
    "    moe_mult: float = 2.667\n",
    "\n",
    "    def setup(self):\n",
    "        self.tok_embed = nn.Embed(self.vocab_size, self.d_model)\n",
    "        self.blocks = [\n",
    "            DecoderBlockMoE(\n",
    "                self.d_model, self.n_heads,\n",
    "                attn_dropout=self.dropout,\n",
    "                moe_dropout=self.dropout,\n",
    "                moe_mult=self.moe_mult,\n",
    "                n_experts=self.n_experts\n",
    "            )\n",
    "            for _ in range(self.n_layers)\n",
    "        ]\n",
    "        self.final_norm = RMSNorm(self.d_model)\n",
    "\n",
    "    def __call__(self, idx, *, deterministic: bool = True, pad_mask: jnp.ndarray | None = None):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_embed(idx)\n",
    "        x = nn.Dropout(self.dropout)(x, deterministic=deterministic)\n",
    "\n",
    "        causal = attn.make_causal_mask(jnp.ones((B, T), dtype=bool))\n",
    "        mask = causal if pad_mask is None else attn.combine_masks(\n",
    "            causal, attn.make_attention_mask(pad_mask, pad_mask, dtype=bool)\n",
    "        )\n",
    "\n",
    "        aux_total = jnp.array(0.0, dtype=x.dtype)\n",
    "        for blk in self.blocks:\n",
    "            x, aux = blk(x, mask=mask, deterministic=deterministic)\n",
    "            aux_total = aux_total + aux\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = jnp.einsum('btd,vd->btv', x, self.tok_embed.embedding)\n",
    "        return logits, aux_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adacf9",
   "metadata": {},
   "source": [
    "### Relative Position Bias + Local-Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen import attention as attn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    dim: int\n",
    "    eps: float = 1e-8\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        g = self.param('scale', nn.initializers.ones, (self.dim,))\n",
    "        rms = jnp.sqrt(jnp.mean(x * x, axis=-1, keepdims=True) + self.eps)\n",
    "        return x * (g / rms)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    d_model: int\n",
    "    mult: float = 2.667\n",
    "    dropout: float = 0.1\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        h = int(self.d_model * self.mult)\n",
    "        u = nn.Dense(h, use_bias=False)(x)\n",
    "        v = nn.Dense(h, use_bias=False)(x)\n",
    "        y = nn.silu(u) * v\n",
    "        y = nn.Dropout(self.dropout)(y, deterministic=deterministic)\n",
    "        return nn.Dense(self.d_model, use_bias=False)(y)\n",
    "\n",
    "def relative_position_bucket(rel_pos, bidirectional: bool, num_buckets=32, max_distance=128):\n",
    "    n = num_buckets // (2 if bidirectional else 1)\n",
    "    sign = (rel_pos < 0).astype(jnp.int32) if bidirectional else 0\n",
    "    rp = jnp.abs(rel_pos)\n",
    "    max_exact = n // 2\n",
    "    is_small = rp < max_exact\n",
    "    val_large = max_exact + (jnp.log(rp / max_exact + 1e-6) / jnp.log(max_distance / max_exact)) * (n - max_exact)\n",
    "    val_large = val_large.astype(jnp.int32)\n",
    "    val_large = jnp.minimum(val_large, n - 1)\n",
    "    buckets = jnp.where(is_small, rp, val_large)\n",
    "    return buckets + sign * n\n",
    "\n",
    "class RelPosBias(nn.Module):\n",
    "    num_buckets: int\n",
    "    num_heads: int\n",
    "    max_distance: int\n",
    "    bidirectional: bool = False\n",
    "    @nn.compact\n",
    "    def __call__(self, qlen, klen):\n",
    "        cp = jnp.arange(qlen)[:, None]\n",
    "        mp = jnp.arange(klen)[None, :]\n",
    "        rel = mp - cp\n",
    "        buckets = relative_position_bucket(rel, self.bidirectional, self.num_buckets, self.max_distance)\n",
    "        table = self.param('bias', nn.initializers.normal(0.02),\n",
    "                           (self.num_buckets * (2 if self.bidirectional else 1), self.num_heads))\n",
    "        bias = table[buckets]\n",
    "        return bias.transpose(2, 0, 1)[None, ...]\n",
    "\n",
    "class LocalGlobalSelfAttention(nn.Module):\n",
    "    d_model: int\n",
    "    n_heads: int\n",
    "    window: int = 512\n",
    "    n_global: int = 8\n",
    "    attn_dropout: float = 0.1\n",
    "    num_buckets: int = 32\n",
    "    max_distance: int = 512\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        B, T, D = x.shape\n",
    "        H = self.n_heads\n",
    "        Dh = D // H\n",
    "\n",
    "        g_tokens = self.param('globals', nn.initializers.normal(0.02), (self.n_global, D))\n",
    "        g = jnp.broadcast_to(g_tokens[None, :, :], (B, self.n_global, D))\n",
    "        cat = jnp.concatenate([g, x], axis=1)  # (B, G+T, D)\n",
    "        G = self.n_global\n",
    "\n",
    "        q = nn.Dense(D, use_bias=False)(x).reshape(B, T, H, Dh).transpose(0, 2, 1, 3)\n",
    "        k = nn.Dense(D, use_bias=False)(cat).reshape(B, G + T, H, Dh).transpose(0, 2, 1, 3)\n",
    "        v = nn.Dense(D, use_bias=False)(cat).reshape(B, G + T, H, Dh).transpose(0, 2, 1, 3)\n",
    "\n",
    "        rp_bias = RelPosBias(num_buckets=self.num_buckets, num_heads=H, max_distance=self.max_distance)(T, T)\n",
    "        zero_g = jnp.zeros((1, H, T, G), dtype=q.dtype)\n",
    "        rel_bias = jnp.concatenate([zero_g, rp_bias], axis=-1)\n",
    "\n",
    "        ar = jnp.arange(T)\n",
    "        dist = ar[None, :] - ar[:, None]\n",
    "        local = (dist >= -self.window) & (dist <= 0)\n",
    "        mask_seq = local[None, None, :, :]\n",
    "        mask = jnp.concatenate([jnp.ones((1,1,T,G), dtype=bool), mask_seq], axis=-1)\n",
    "\n",
    "        logits = jnp.einsum('bhtd,bhkd->bhtk', q, k) / jnp.sqrt(Dh) + rel_bias\n",
    "        logits = jnp.where(mask, logits, jnp.finfo(logits.dtype).min)\n",
    "        att = nn.softmax(logits, axis=-1)\n",
    "        att = nn.Dropout(self.attn_dropout)(att, deterministic=deterministic)\n",
    "        y = jnp.einsum('bhtk,bhkd->bhtd', att, v)\n",
    "        y = y.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "        y = nn.Dense(D, use_bias=False)(y)\n",
    "        return y\n",
    "\n",
    "class DecoderBlockLocalGlobal(nn.Module):\n",
    "    d_model: int; n_heads: int\n",
    "    window: int = 512; n_global: int = 8\n",
    "    attn_dropout: float = 0.1; mlp_dropout: float = 0.1\n",
    "    resid_scale_init: float = 1e-2\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool = True):\n",
    "        scale = self.param('res_scale', nn.initializers.constant(self.resid_scale_init), ())\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = LocalGlobalSelfAttention(self.d_model, self.n_heads, self.window, self.n_global,\n",
    "                                     attn_dropout=self.attn_dropout)(h, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = SwiGLU(self.d_model, mult=2.667, dropout=self.mlp_dropout)(h, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "        return x\n",
    "\n",
    "class RelPosLocalGlobalTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    window: int = 512\n",
    "    n_global: int = 8\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    def setup(self):\n",
    "        self.tok_embed = nn.Embed(self.vocab_size, self.d_model)\n",
    "        self.blocks = [DecoderBlockLocalGlobal(self.d_model, self.n_heads, self.window, self.n_global,\n",
    "                                               attn_dropout=self.dropout, mlp_dropout=self.dropout)\n",
    "                       for _ in range(self.n_layers)]\n",
    "        self.final_norm = RMSNorm(self.d_model)\n",
    "\n",
    "    def __call__(self, idx, *, deterministic: bool = True):\n",
    "        x = self.tok_embed(idx)\n",
    "        x = nn.Dropout(self.dropout)(x, deterministic=deterministic)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, deterministic=deterministic)\n",
    "        x = self.final_norm(x)\n",
    "        logits = jnp.einsum('btd,vd->btv', x, self.tok_embed.embedding)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d96ba6",
   "metadata": {},
   "source": [
    "### standard causal decoder (not so different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen import attention as attn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    dim: int\n",
    "    eps: float = 1e-8\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        g = self.param('scale', nn.initializers.ones, (self.dim,))\n",
    "        rms = jnp.sqrt(jnp.mean(x * x, axis=-1, keepdims=True) + self.eps)\n",
    "        return x * (g / rms)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    d_model: int\n",
    "    mult: float = 2.667\n",
    "    dropout: float = 0.1\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        h = int(self.d_model * self.mult)\n",
    "        u = nn.Dense(h, use_bias=False)(x)\n",
    "        v = nn.Dense(h, use_bias=False)(x)\n",
    "        y = nn.silu(u) * v\n",
    "        y = nn.Dropout(self.dropout)(y, deterministic=deterministic)\n",
    "        return nn.Dense(self.d_model, use_bias=False)(y)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    d_model: int; n_heads: int\n",
    "    attn_dropout: float = 0.1; mlp_dropout: float = 0.1\n",
    "    resid_scale_init: float = 1e-2\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, mask=None, deterministic: bool = True):\n",
    "        scale = self.param('res_scale', nn.initializers.constant(self.resid_scale_init), ())\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = nn.SelfAttention(num_heads=self.n_heads, use_bias=False, dropout_rate=self.attn_dropout)(\n",
    "            h, mask=mask, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "        h = RMSNorm(self.d_model)(x)\n",
    "        h = SwiGLU(self.d_model, dropout=self.mlp_dropout)(h, deterministic=deterministic)\n",
    "        x = x + scale * h\n",
    "        return x\n",
    "\n",
    "class NgramAuxTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    def setup(self):\n",
    "        self.tok_embed = nn.Embed(self.vocab_size, self.d_model)\n",
    "        self.blocks = [DecoderBlock(self.d_model, self.n_heads, self.dropout, self.dropout)\n",
    "                       for _ in range(self.n_layers)]\n",
    "        self.final_norm = RMSNorm(self.d_model)\n",
    "\n",
    "    def __call__(self, idx, *, deterministic: bool = True, pad_mask: jnp.ndarray | None = None):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_embed(idx)\n",
    "        x = nn.Dropout(self.dropout)(x, deterministic=deterministic)\n",
    "\n",
    "        causal = attn.make_causal_mask(jnp.ones((B, T), dtype=bool))\n",
    "        mask = causal if pad_mask is None else attn.combine_masks(\n",
    "            causal, attn.make_attention_mask(pad_mask, pad_mask, dtype=bool))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=mask, deterministic=deterministic)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        E = self.tok_embed.embedding\n",
    "        logits1 = jnp.einsum('btd,vd->btv', x, E)\n",
    "        logits2 = logits1\n",
    "        logits3 = logits1\n",
    "        return {'logits1': logits1, 'logits2': logits2, 'logits3': logits3}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
